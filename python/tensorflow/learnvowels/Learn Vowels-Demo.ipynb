{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Practice problem!  yay\n",
    "#\n",
    "# Our mission, should we choose to accept it, is to see if we can use supervised machine learning to train\n",
    "# a deep neural network to count the number of vowels in words it has never seen before, without ever telling\n",
    "# neural network which letters are vowels!\n",
    "#\n",
    "# This is best done algorithmically with a few lines of code and no neural networks\n",
    "# but that doesn't matter - it's a practice problem and anything goes :)  \n",
    "# Also, we only count a,e,i,o,u as vowels (sorry about that y, nothing personal).\n",
    "#\n",
    "# The data set consists of just over 10k words in random order.  Each word has a maximum of 10 characters.\n",
    "# The first row is the header, and there are five columns (the first is the row index):\n",
    "#\n",
    "#   1st column:         - the row index\n",
    "#   2nd column:  a2i    - the word, in english\n",
    "#   3rd column:  vowels - the number of vowels in the word\n",
    "#   4th column:  binary - the word, encoded into 70 binary digits, left-padded with 0, with 7 bits per character.\n",
    "#                         For example, the word 'vital' is \n",
    "#                         encoded as 0000000000000000000000000000000000011101101101001111010011000011101100\n",
    "#\n",
    "\n",
    "\n",
    "# Special thanks to the UCI Machine Learning Repository who provided the data set that this one was based on\n",
    "# For similar datasets, please see https://archive.ics.uci.edu/ml/datasets/bag+of+words\n",
    "# Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the original collection of words\n",
    "df = pd.read_csv('vocab.nips.txt')\n",
    "\n",
    "# Randomize the list\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Keep the words that have 10 or fewer characters\n",
    "df = df[df.a2i.str.len() <= 10]\n",
    "\n",
    "# Count the number of vowels in each word and store them in the 'vowels' column.\n",
    "def countVowels(row):\n",
    "    s = row['a2i']\n",
    "    return s.count('a') + s.count('e') + s.count('i') + s.count('o') + s.count('u')\n",
    "df['vowels'] = df.apply(countVowels, axis=1)\n",
    "\n",
    "# Create a column to hold the binary representation of each word (using ASCII 0s and 1s to represent the ASCII 7-bit binary code of each letter)\n",
    "df['binary'] = df.apply(lambda row: ''.join(format(ord(x), 'b') for x in row[0]).rjust(70, '0'), axis=1)\n",
    "\n",
    "# Save the new dataset as a file\n",
    "df.to_csv('vocab.vowels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell loads the data set and splits it into numpy arrays of labels and examples, \n",
    "# for both training and evalutation.\n",
    "\n",
    "# The train_data, and train_labels are used for training the neural net.  The train_words is just there for convenience.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('vocab.vowels.csv', names=['id', 'word', 'vowels', 'binary'], index_col='id', skiprows=1)\n",
    "rows = df.shape[0]\n",
    "train_rows = int(rows * 0.7)\n",
    "\n",
    "df_train, df_eval = df.iloc[:train_rows, :], df.iloc[train_rows:, :]\n",
    "\n",
    "train_list = [[int(c) for c in i] for i in df_train['binary']]\n",
    "eval_list  = [[int(c) for c in i] for i in df_eval['binary'] ]\n",
    "\n",
    "train_data   = np.array(train_list).astype(np.float32)\n",
    "eval_data    = np.array(eval_list).astype(np.float32)\n",
    "\n",
    "train_words  = df_train.iloc[:,0].values\n",
    "eval_words   = df_eval.iloc[:,0].values\n",
    "\n",
    "train_labels = df_train.iloc[:,1].values.astype(np.float32)\n",
    "eval_labels  = df_eval.iloc[:,1].values.astype(np.float32)\n",
    "\n",
    "print(\"Total rows:      \" + str(rows))\n",
    "print(\"Training rows:   \" + str(train_rows))\n",
    "print(\"Evaluation rows: \" + str(rows - train_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just print some example data\n",
    "i = 1131\n",
    "print(train_words[i])\n",
    "print(train_labels[i])\n",
    "print(train_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "dropout_rate = 0.25\n",
    "\n",
    "\n",
    "\n",
    "def convolution(mode, input_layer, filters, kernel_size, strides=1, padding=\"VALID\", normalize=False):\n",
    "    \"\"\"\n",
    "    Creates a convolutional layer with the given number of \n",
    "    filters and kernel size, with optional batch normalization\n",
    "    \"\"\"\n",
    "    layer = tf.layers.conv1d(\n",
    "          inputs=input_layer,\n",
    "          kernel_size=kernel_size,\n",
    "          filters=filters,\n",
    "          strides=strides,\n",
    "          padding=padding,\n",
    "          activation=tf.nn.relu)\n",
    "    if normalize:\n",
    "        layer = tf.layers.batch_normalization(layer, training=True)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def deep(mode, layer, units, reshape=None):\n",
    "    \"\"\"\n",
    "    Creates a deep layer with dropout and batch normalization\n",
    "    \"\"\"\n",
    "    if reshape != None:\n",
    "        layer = tf.reshape(layer, reshape)\n",
    "    layer = tf.layers.dropout(inputs=layer, rate=dropout_rate, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    layer = tf.layers.dense(inputs=layer, units=units, activation=tf.nn.relu)\n",
    "    layer = tf.layers.batch_normalization(layer, training=True)\n",
    "    return layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def model_fn(features, labels, mode):\n",
    "    \"\"\"Creates the neural network model\"\"\"\n",
    "\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "\n",
    "        # Input Layer\n",
    "        input = tf.reshape(features[\"x\"], [-1, 70])\n",
    "        num_outputs = 11\n",
    "\n",
    "        # Send the input through fully connected layers (FC layers)  #deeplearning\n",
    "        layer1  = deep(mode, input, 128)\n",
    "        layer2  = deep(mode, layer1, 64)\n",
    "        layer3  = deep(mode, layer2, 32)\n",
    "        \n",
    "        \n",
    "        # Classification Layer (there are 11 possible outputs)\n",
    "        logits = tf.layers.dense(inputs=layer3, units=num_outputs, name=\"output_layer\")\n",
    "        \n",
    "        output = {\n",
    "          \"classes\"      : tf.argmax(input=logits, axis=1),\n",
    "          \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\"),\n",
    "          \"layer1\"       : layer1, \n",
    "          \"layer2\"       : layer2,\n",
    "          \"layer3\"       : layer3\n",
    "        }\n",
    "        \n",
    "\n",
    "        loss=None\n",
    "        train_op = None\n",
    "        eval_metric_ops = None\n",
    "        \n",
    "        # i.e. for both TRAIN and EVAL modes\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "\n",
    "            onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=num_outputs)\n",
    "\n",
    "            # Calculate the loss (using cross-entropy)\n",
    "            loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "            # Minimize the loss\n",
    "            train_op = tf.train.GradientDescentOptimizer(0.01).minimize(\n",
    "                loss=loss, global_step=tf.train.get_global_step())\n",
    "            \n",
    "            # Gather some metrics\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            \n",
    "            eval_metric_ops = {\n",
    "              \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=output[\"classes\"])\n",
    "            }\n",
    "\n",
    "        tf.summary.merge_all()\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, \n",
    "                                          loss=loss,\n",
    "                                          train_op=train_op,\n",
    "                                          eval_metric_ops=eval_metric_ops,\n",
    "                                          predictions=output)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"demo10\"\n",
    "\n",
    "\n",
    "input_fn = {\n",
    "        # Train the model\n",
    "    \"training\" : tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": train_data},\n",
    "        y=train_labels,\n",
    "        batch_size=64,\n",
    "        num_epochs=None,\n",
    "        shuffle=True),\n",
    "    \n",
    "    \"evaluation\" : tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": eval_data},\n",
    "        y=eval_labels,\n",
    "        batch_size=1024,\n",
    "        num_epochs=None,\n",
    "        shuffle=True),\n",
    "\n",
    "    \"prediction\" : tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": eval_data},\n",
    "        y=eval_labels,\n",
    "        batch_size=len(eval_data),\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "\n",
    "}\n",
    "\n",
    "def trainTheModel(input_fn):\n",
    "    \"\"\"\n",
    "    Expects an input_fn dict containing tensorflow input functions with the names\n",
    "    \"training\", \"evaluation\", \"prediction\"\n",
    "    \"\"\"\n",
    "    # Create the Estimator\n",
    "    session_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "    session_config.gpu_options.per_process_gpu_memory_fraction = 0.75\n",
    "    \n",
    "    run_config = tf.estimator.RunConfig()\n",
    "    run_config = run_config.replace(\n",
    "        save_checkpoints_steps=200, \n",
    "        session_config=session_config,\n",
    "        keep_checkpoint_max=100)\n",
    "\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=model_fn, model_dir=model_dir, config=run_config)\n",
    "\n",
    "    \n",
    "    for _ in range(5):\n",
    "        estimator.train(input_fn=input_fn[\"training\"], steps=1000)\n",
    "        estimator.evaluate(input_fn=input_fn[\"evaluation\"], steps=1)\n",
    "        \n",
    "    \n",
    "trainTheModel(input_fn)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Strategy:\n",
    "# Use the estimator to run predictions with 500 elements from the set.\n",
    "# Get back an iterator of the predictions\n",
    "# Extract prediction tensors (the embeddings of various layers) as a bunch of numpy arrays\n",
    "# Create a new session (and new graph).\n",
    "# Create tf.Variable objects in the new session, initialized to the numpy arrays\n",
    "# Add the variables to a projector config\n",
    "# Run the session (try tf.global_variables_initializer() or a variant)\n",
    "# Save the graph and a checkpoint\n",
    "# View them in TensorBoard\n",
    "\n",
    "\n",
    "samples = 1500\n",
    "\n",
    "# We'll use this to perform the training\n",
    "path = os.path.join(model_dir, \"outputs\")\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "with open(os.path.join(path, 'labels.tsv'),'w') as f:\n",
    "    f.write(\"Index\\tLabel\\tVowels\\tLetters\\n\")\n",
    "    for index,label in enumerate(eval_labels):\n",
    "        if index >= samples:\n",
    "            break\n",
    "        f.write(\"%d\\t%s\\t%d\\t%d\\n\" % (index,eval_words[index]+str(label),int(label),len(eval_words[index])))\n",
    "        \n",
    "\n",
    "def save_outputs(input_fn, model_fn, model_dir, layer_names, samples):\n",
    "\n",
    "    # Create the estimator\n",
    "    estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir)\n",
    "\n",
    "    # Run a prediction\n",
    "    prediction = estimator.predict(input_fn=input_fn)\n",
    "    \n",
    "\n",
    "    # Loop through the results and create a dict of the example outputs from each desired layer\n",
    "    i = 0\n",
    "    layers = {}\n",
    "    for name in layer_names:\n",
    "        layers[name] = []\n",
    "    for p in prediction:\n",
    "        i = i + 1\n",
    "        if i > samples:\n",
    "            break;\n",
    "        for name in layer_names:\n",
    "            layers[name].append(p[name])\n",
    "    \n",
    "    # Convert to a numpy array, \n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.Session() as sess:\n",
    "            tfvars = []\n",
    "            for name in layer_names:\n",
    "                v = tf.get_variable(name, np.shape(layers[name]), initializer=tf.zeros_initializer)\n",
    "                va = tf.assign(v,  np.array(layers[name]), name=name)\n",
    "                tfvars.append(va)\n",
    "            print(tfvars[0])\n",
    "            sess.run(tfvars)\n",
    "            tf.train.Saver().save(sess, os.path.join(path, \"outputs.ckpt\"))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "save_outputs(input_fn[\"prediction\"], model_fn, model_dir, [\"layer1\", \"layer2\", \"layer3\"], samples)\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#        # Convolutional layers\n",
    "#        conv0 = convolution(mode, input, 52, 7, strides=7, padding=\"VALID\")\n",
    "#        flat  = tf.reshape(conv0, [-1, 520])   # Reshape into a flat array\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
